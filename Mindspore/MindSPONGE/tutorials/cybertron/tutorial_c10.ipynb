{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021-2022 @ Shenzhen Bay Laboratory & Peking University & Huawei Technologies Co., Ltd\n",
    "\n",
    "This code is a part of Cybertron package.\n",
    "\n",
    "The Cybertron is open-source software based on the AI-framework:\n",
    "MindSpore (https://www.mindspore.cn/)\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "\n",
    "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Cybertron tutorial 10: Run MD simulation in with CybertronFF as potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from mindspore import load_checkpoint\n",
    "from mindspore import context\n",
    "\n",
    "from mindsponge import Molecule\n",
    "from mindsponge import Sponge\n",
    "from mindsponge import set_global_units\n",
    "from mindsponge.callback import RunInfo, WriteH5MD\n",
    "from mindsponge.control import LeapFrog\n",
    "from mindsponge.control import Langevin\n",
    "from mindsponge.optimizer import DynamicUpdater\n",
    "\n",
    "from cybertron.model import MolCT\n",
    "from cybertron.readout import AtomwiseReadout\n",
    "from cybertron.cybertron import CybertronFF\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_units('A', 'kcal/mol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_types = np.array(\n",
    "    [[6, 1, 1, 1, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 25]], np.int32)\n",
    "coordinate = np.array([\n",
    "    [0.782936, -0.21384, 1.940403],\n",
    "    [0.90026, -1.258313, 2.084498],\n",
    "    [1.793443, 0.267702, 1.791434],\n",
    "    [0.161631, 0.247471, 2.702921],\n",
    "    [-1.775807, 0.660242, 0.992526],\n",
    "    [-2.573144, 0.82639, 1.806692],\n",
    "    [-0.793238, 0.551875, -1.559148],\n",
    "    [-0.922246, 0.719072, -2.702972],\n",
    "    [1.526357, -0.229486, -0.35567],\n",
    "    [2.624975, -0.473657, -0.641924],\n",
    "    [-0.786405, -1.533853, -0.007962],\n",
    "    [-1.266142, -2.537492, 0.254628],\n",
    "    [0.394547, 1.910025, 0.468161],\n",
    "    [0.747036, 3.027445, 0.458565],\n",
    "    [-0.163356, 0.241977, 0.175396],\n",
    "])\n",
    "system = Molecule(atomic_number=atom_types, coordinate=coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = MolCT(\n",
    "    dim_feature=128,\n",
    "    num_atom_types=100,\n",
    "    n_interaction=3,\n",
    "    n_heads=8,\n",
    "    max_cycles=1,\n",
    "    cutoff=10,\n",
    "    fixed_cycles=True,\n",
    "    length_unit='A',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout = AtomwiseReadout(\n",
    "    model=mod,\n",
    "    dim_output=1,\n",
    "    activation=mod.activation,\n",
    "    scale=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential = CybertronFF(\n",
    "    model=mod,\n",
    "    readout=readout,\n",
    "    atom_types=atom_types,\n",
    "    length_unit='A',\n",
    "    energy_unit='kcal/mol',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.atom_embedding.embedding_table': Parameter (name=model.atom_embedding.embedding_table, shape=(100, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.linear.weight': Parameter (name=model.dis_filter.linear.weight, shape=(128, 64), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.linear.bias': Parameter (name=model.dis_filter.linear.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.residual.nonlinear.mlp.0.weight': Parameter (name=model.dis_filter.residual.nonlinear.mlp.0.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.residual.nonlinear.mlp.0.bias': Parameter (name=model.dis_filter.residual.nonlinear.mlp.0.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.residual.nonlinear.mlp.1.weight': Parameter (name=model.dis_filter.residual.nonlinear.mlp.1.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.dis_filter.residual.nonlinear.mlp.1.bias': Parameter (name=model.dis_filter.residual.nonlinear.mlp.1.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.norm.gamma': Parameter (name=model.interactions.0.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.norm.beta': Parameter (name=model.interactions.0.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.x2q.weight': Parameter (name=model.interactions.0.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.x2k.weight': Parameter (name=model.interactions.0.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.positional_embedding.x2v.weight': Parameter (name=model.interactions.0.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.0.multi_head_attention.output.weight': Parameter (name=model.interactions.0.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.norm.gamma': Parameter (name=model.interactions.1.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.norm.beta': Parameter (name=model.interactions.1.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.x2q.weight': Parameter (name=model.interactions.1.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.x2k.weight': Parameter (name=model.interactions.1.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.positional_embedding.x2v.weight': Parameter (name=model.interactions.1.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.1.multi_head_attention.output.weight': Parameter (name=model.interactions.1.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.norm.gamma': Parameter (name=model.interactions.2.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.norm.beta': Parameter (name=model.interactions.2.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.x2q.weight': Parameter (name=model.interactions.2.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.x2k.weight': Parameter (name=model.interactions.2.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.positional_embedding.x2v.weight': Parameter (name=model.interactions.2.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'model.interactions.2.multi_head_attention.output.weight': Parameter (name=model.interactions.2.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'readout.decoder.output.mlp.0.weight': Parameter (name=readout.decoder.output.mlp.0.weight, shape=(64, 128), dtype=Float32, requires_grad=True),\n",
       " 'readout.decoder.output.mlp.0.bias': Parameter (name=readout.decoder.output.mlp.0.bias, shape=(64,), dtype=Float32, requires_grad=True),\n",
       " 'readout.decoder.output.mlp.1.weight': Parameter (name=readout.decoder.output.mlp.1.weight, shape=(1, 64), dtype=Float32, requires_grad=True),\n",
       " 'readout.decoder.output.mlp.1.bias': Parameter (name=readout.decoder.output.mlp.1.bias, shape=(1,), dtype=Float32, requires_grad=True),\n",
       " 'step': Parameter (name=step, shape=(), dtype=Int32, requires_grad=True),\n",
       " 'global_step': Parameter (name=global_step, shape=(1,), dtype=Int32, requires_grad=True),\n",
       " 'beta1_power': Parameter (name=beta1_power, shape=(1,), dtype=Float32, requires_grad=True),\n",
       " 'beta2_power': Parameter (name=beta2_power, shape=(1,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.atom_embedding.embedding_table': Parameter (name=moment1.model.atom_embedding.embedding_table, shape=(100, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.linear.weight': Parameter (name=moment1.model.dis_filter.linear.weight, shape=(128, 64), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.linear.bias': Parameter (name=moment1.model.dis_filter.linear.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.residual.nonlinear.mlp.0.weight': Parameter (name=moment1.model.dis_filter.residual.nonlinear.mlp.0.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.residual.nonlinear.mlp.0.bias': Parameter (name=moment1.model.dis_filter.residual.nonlinear.mlp.0.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.residual.nonlinear.mlp.1.weight': Parameter (name=moment1.model.dis_filter.residual.nonlinear.mlp.1.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.dis_filter.residual.nonlinear.mlp.1.bias': Parameter (name=moment1.model.dis_filter.residual.nonlinear.mlp.1.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.norm.gamma': Parameter (name=moment1.model.interactions.0.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.norm.beta': Parameter (name=moment1.model.interactions.0.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.x2q.weight': Parameter (name=moment1.model.interactions.0.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.x2k.weight': Parameter (name=moment1.model.interactions.0.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.positional_embedding.x2v.weight': Parameter (name=moment1.model.interactions.0.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.0.multi_head_attention.output.weight': Parameter (name=moment1.model.interactions.0.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.norm.gamma': Parameter (name=moment1.model.interactions.1.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.norm.beta': Parameter (name=moment1.model.interactions.1.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.x2q.weight': Parameter (name=moment1.model.interactions.1.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.x2k.weight': Parameter (name=moment1.model.interactions.1.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.positional_embedding.x2v.weight': Parameter (name=moment1.model.interactions.1.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.1.multi_head_attention.output.weight': Parameter (name=moment1.model.interactions.1.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.norm.gamma': Parameter (name=moment1.model.interactions.2.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.norm.beta': Parameter (name=moment1.model.interactions.2.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.x2q.weight': Parameter (name=moment1.model.interactions.2.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.x2k.weight': Parameter (name=moment1.model.interactions.2.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.positional_embedding.x2v.weight': Parameter (name=moment1.model.interactions.2.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.model.interactions.2.multi_head_attention.output.weight': Parameter (name=moment1.model.interactions.2.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.readout.decoder.output.mlp.0.weight': Parameter (name=moment1.readout.decoder.output.mlp.0.weight, shape=(64, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment1.readout.decoder.output.mlp.0.bias': Parameter (name=moment1.readout.decoder.output.mlp.0.bias, shape=(64,), dtype=Float32, requires_grad=True),\n",
       " 'moment1.readout.decoder.output.mlp.1.weight': Parameter (name=moment1.readout.decoder.output.mlp.1.weight, shape=(1, 64), dtype=Float32, requires_grad=True),\n",
       " 'moment1.readout.decoder.output.mlp.1.bias': Parameter (name=moment1.readout.decoder.output.mlp.1.bias, shape=(1,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.atom_embedding.embedding_table': Parameter (name=moment2.model.atom_embedding.embedding_table, shape=(100, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.linear.weight': Parameter (name=moment2.model.dis_filter.linear.weight, shape=(128, 64), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.linear.bias': Parameter (name=moment2.model.dis_filter.linear.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.residual.nonlinear.mlp.0.weight': Parameter (name=moment2.model.dis_filter.residual.nonlinear.mlp.0.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.residual.nonlinear.mlp.0.bias': Parameter (name=moment2.model.dis_filter.residual.nonlinear.mlp.0.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.residual.nonlinear.mlp.1.weight': Parameter (name=moment2.model.dis_filter.residual.nonlinear.mlp.1.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.dis_filter.residual.nonlinear.mlp.1.bias': Parameter (name=moment2.model.dis_filter.residual.nonlinear.mlp.1.bias, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.norm.gamma': Parameter (name=moment2.model.interactions.0.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.norm.beta': Parameter (name=moment2.model.interactions.0.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.x2q.weight': Parameter (name=moment2.model.interactions.0.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.x2k.weight': Parameter (name=moment2.model.interactions.0.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.positional_embedding.x2v.weight': Parameter (name=moment2.model.interactions.0.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.0.multi_head_attention.output.weight': Parameter (name=moment2.model.interactions.0.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.norm.gamma': Parameter (name=moment2.model.interactions.1.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.norm.beta': Parameter (name=moment2.model.interactions.1.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.x2q.weight': Parameter (name=moment2.model.interactions.1.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.x2k.weight': Parameter (name=moment2.model.interactions.1.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.positional_embedding.x2v.weight': Parameter (name=moment2.model.interactions.1.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.1.multi_head_attention.output.weight': Parameter (name=moment2.model.interactions.1.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.norm.gamma': Parameter (name=moment2.model.interactions.2.positional_embedding.norm.gamma, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.norm.beta': Parameter (name=moment2.model.interactions.2.positional_embedding.norm.beta, shape=(128,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.x2q.weight': Parameter (name=moment2.model.interactions.2.positional_embedding.x2q.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.x2k.weight': Parameter (name=moment2.model.interactions.2.positional_embedding.x2k.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.positional_embedding.x2v.weight': Parameter (name=moment2.model.interactions.2.positional_embedding.x2v.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.model.interactions.2.multi_head_attention.output.weight': Parameter (name=moment2.model.interactions.2.multi_head_attention.output.weight, shape=(128, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.readout.decoder.output.mlp.0.weight': Parameter (name=moment2.readout.decoder.output.mlp.0.weight, shape=(64, 128), dtype=Float32, requires_grad=True),\n",
       " 'moment2.readout.decoder.output.mlp.0.bias': Parameter (name=moment2.readout.decoder.output.mlp.0.bias, shape=(64,), dtype=Float32, requires_grad=True),\n",
       " 'moment2.readout.decoder.output.mlp.1.weight': Parameter (name=moment2.readout.decoder.output.mlp.1.weight, shape=(1, 64), dtype=Float32, requires_grad=True),\n",
       " 'moment2.readout.decoder.output.mlp.1.bias': Parameter (name=moment2.readout.decoder.output.mlp.1.bias, shape=(1,), dtype=Float32, requires_grad=True)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_file = 'checkpoint_c10.ckpt'\n",
    "load_checkpoint(param_file, net=potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = DynamicUpdater(\n",
    "    system,\n",
    "    integrator=LeapFrog(system),\n",
    "    thermostat=Langevin(system, 300),\n",
    "    time_step=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58.57325]]\n"
     ]
    }
   ],
   "source": [
    "md = Sponge(system, potential, opt)\n",
    "print(md.energy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_h5md = WriteH5MD(system, 'Tutorial_C10.h5md', save_freq=10)\n",
    "cb_sim = RunInfo(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, E_pot: 58.57325, E_kin: 0.0, E_tot: 58.57325, Temperature: 0.0\n",
      "Step: 10, E_pot: 58.532314, E_kin: 0.050171684, E_tot: 58.582485, Temperature: 1.2022558\n",
      "Step: 20, E_pot: 58.412285, E_kin: 0.19198488, E_tot: 58.60427, Temperature: 4.600502\n",
      "Step: 30, E_pot: 58.20153, E_kin: 0.42298678, E_tot: 58.624516, Temperature: 10.1359625\n",
      "Step: 40, E_pot: 57.883507, E_kin: 0.7422206, E_tot: 58.62573, Temperature: 17.78571\n",
      "Step: 50, E_pot: 57.4255, E_kin: 1.2067266, E_tot: 58.632225, Temperature: 28.91659\n",
      "Step: 60, E_pot: 56.812164, E_kin: 1.7930272, E_tot: 58.60519, Temperature: 42.966015\n",
      "Step: 70, E_pot: 56.098404, E_kin: 2.4371324, E_tot: 58.535538, Temperature: 58.400604\n",
      "Step: 80, E_pot: 55.401566, E_kin: 3.1552405, E_tot: 58.556805, Temperature: 75.60851\n",
      "Step: 90, E_pot: 54.823933, E_kin: 3.7234538, E_tot: 58.547386, Temperature: 89.22451\n",
      "Step: 100, E_pot: 54.35311, E_kin: 4.209216, E_tot: 58.56233, Temperature: 100.86475\n",
      "Step: 110, E_pot: 53.92499, E_kin: 4.6739516, E_tot: 58.59894, Temperature: 112.00114\n",
      "Step: 120, E_pot: 53.465096, E_kin: 5.083965, E_tot: 58.54906, Temperature: 121.82622\n",
      "Step: 130, E_pot: 52.89983, E_kin: 5.8187823, E_tot: 58.718613, Temperature: 139.43454\n",
      "Step: 140, E_pot: 52.202156, E_kin: 6.3968544, E_tot: 58.59901, Temperature: 153.28676\n",
      "Step: 150, E_pot: 51.39827, E_kin: 7.2756248, E_tot: 58.673893, Temperature: 174.3446\n",
      "Step: 160, E_pot: 50.470566, E_kin: 8.261587, E_tot: 58.732155, Temperature: 197.97107\n",
      "Step: 170, E_pot: 49.195126, E_kin: 9.635178, E_tot: 58.830303, Temperature: 230.88618\n",
      "Step: 180, E_pot: 47.771275, E_kin: 10.99514, E_tot: 58.766415, Temperature: 263.47473\n",
      "Step: 190, E_pot: 46.680706, E_kin: 12.209347, E_tot: 58.890053, Temperature: 292.5706\n",
      "Step: 200, E_pot: 45.549664, E_kin: 13.520553, E_tot: 59.070217, Temperature: 323.99078\n",
      "Step: 210, E_pot: 44.389847, E_kin: 14.555417, E_tot: 58.945263, Temperature: 348.78906\n",
      "Step: 220, E_pot: 43.323055, E_kin: 15.533335, E_tot: 58.85639, Temperature: 372.22275\n",
      "Step: 230, E_pot: 42.32419, E_kin: 16.435528, E_tot: 58.759716, Temperature: 393.84183\n",
      "Step: 240, E_pot: 41.46806, E_kin: 17.032835, E_tot: 58.500893, Temperature: 408.15503\n",
      "Step: 250, E_pot: 40.98053, E_kin: 17.630878, E_tot: 58.61141, Temperature: 422.48584\n",
      "Step: 260, E_pot: 40.9718, E_kin: 17.854578, E_tot: 58.826378, Temperature: 427.8463\n",
      "Step: 270, E_pot: 41.34048, E_kin: 17.750137, E_tot: 59.09062, Temperature: 425.34363\n",
      "Step: 280, E_pot: 41.916245, E_kin: 17.331161, E_tot: 59.247406, Temperature: 415.30377\n",
      "Step: 290, E_pot: 42.55424, E_kin: 16.685627, E_tot: 59.23987, Temperature: 399.83493\n",
      "Step: 300, E_pot: 43.10518, E_kin: 16.333536, E_tot: 59.438713, Temperature: 391.39786\n",
      "Step: 310, E_pot: 43.424, E_kin: 15.995088, E_tot: 59.419086, Temperature: 383.28766\n",
      "Step: 320, E_pot: 43.500755, E_kin: 15.922868, E_tot: 59.423622, Temperature: 381.55707\n",
      "Step: 330, E_pot: 43.497826, E_kin: 16.168829, E_tot: 59.666656, Temperature: 387.451\n",
      "Step: 340, E_pot: 43.233578, E_kin: 16.620373, E_tot: 59.85395, Temperature: 398.27127\n",
      "Step: 350, E_pot: 42.437622, E_kin: 17.629097, E_tot: 60.06672, Temperature: 422.44315\n",
      "Step: 360, E_pot: 41.17578, E_kin: 18.883202, E_tot: 60.058983, Temperature: 452.49506\n",
      "Step: 370, E_pot: 39.545334, E_kin: 20.574566, E_tot: 60.1199, Temperature: 493.02493\n",
      "Step: 380, E_pot: 37.775784, E_kin: 22.327955, E_tot: 60.103737, Temperature: 535.04114\n",
      "Step: 390, E_pot: 36.28006, E_kin: 23.740734, E_tot: 60.020794, Temperature: 568.8953\n",
      "Step: 400, E_pot: 35.200096, E_kin: 24.91994, E_tot: 60.120037, Temperature: 597.15247\n",
      "Step: 410, E_pot: 34.299164, E_kin: 25.955507, E_tot: 60.25467, Temperature: 621.9676\n",
      "Step: 420, E_pot: 33.19763, E_kin: 27.027317, E_tot: 60.22495, Temperature: 647.6511\n",
      "Step: 430, E_pot: 31.708458, E_kin: 28.271893, E_tot: 59.98035, Temperature: 677.47473\n",
      "Step: 440, E_pot: 29.944256, E_kin: 30.167183, E_tot: 60.11144, Temperature: 722.89124\n",
      "Step: 450, E_pot: 28.006905, E_kin: 32.4655, E_tot: 60.472404, Temperature: 777.96545\n",
      "Step: 460, E_pot: 26.324726, E_kin: 33.94393, E_tot: 60.268658, Temperature: 813.3929\n",
      "Step: 470, E_pot: 25.192986, E_kin: 35.12432, E_tot: 60.317307, Temperature: 841.67834\n",
      "Step: 480, E_pot: 24.416477, E_kin: 35.982487, E_tot: 60.398964, Temperature: 862.24243\n",
      "Step: 490, E_pot: 23.726618, E_kin: 36.646683, E_tot: 60.3733, Temperature: 878.15845\n",
      "Step: 500, E_pot: 22.987198, E_kin: 37.1412, E_tot: 60.1284, Temperature: 890.0085\n",
      "Step: 510, E_pot: 22.298817, E_kin: 37.708782, E_tot: 60.0076, Temperature: 903.6094\n",
      "Step: 520, E_pot: 21.768425, E_kin: 38.261253, E_tot: 60.02968, Temperature: 916.84814\n",
      "Step: 530, E_pot: 21.559593, E_kin: 38.868584, E_tot: 60.428177, Temperature: 931.4015\n",
      "Step: 540, E_pot: 21.918888, E_kin: 38.453407, E_tot: 60.372295, Temperature: 921.4527\n",
      "Step: 550, E_pot: 22.666998, E_kin: 37.658054, E_tot: 60.32505, Temperature: 902.39374\n",
      "Step: 560, E_pot: 23.42133, E_kin: 37.223095, E_tot: 60.644424, Temperature: 891.9709\n",
      "Step: 570, E_pot: 24.03632, E_kin: 36.573982, E_tot: 60.610302, Temperature: 876.4163\n",
      "Step: 580, E_pot: 24.428299, E_kin: 36.321636, E_tot: 60.749935, Temperature: 870.3694\n",
      "Step: 590, E_pot: 24.546227, E_kin: 35.756226, E_tot: 60.302452, Temperature: 856.82056\n",
      "Step: 600, E_pot: 24.537249, E_kin: 35.887108, E_tot: 60.424355, Temperature: 859.95685\n",
      "Step: 610, E_pot: 24.6739, E_kin: 36.129208, E_tot: 60.80311, Temperature: 865.7583\n",
      "Step: 620, E_pot: 25.039333, E_kin: 35.938538, E_tot: 60.97787, Temperature: 861.1893\n",
      "Step: 630, E_pot: 25.413933, E_kin: 35.625156, E_tot: 61.03909, Temperature: 853.6798\n",
      "Step: 640, E_pot: 25.622252, E_kin: 35.25065, E_tot: 60.872902, Temperature: 844.7055\n",
      "Step: 650, E_pot: 25.786716, E_kin: 35.082275, E_tot: 60.86899, Temperature: 840.67084\n",
      "Step: 660, E_pot: 25.983221, E_kin: 34.967644, E_tot: 60.950867, Temperature: 837.9239\n",
      "Step: 670, E_pot: 26.09483, E_kin: 34.62346, E_tot: 60.71829, Temperature: 829.6763\n",
      "Step: 680, E_pot: 25.942158, E_kin: 34.958572, E_tot: 60.90073, Temperature: 837.70654\n",
      "Step: 690, E_pot: 25.539497, E_kin: 35.39235, E_tot: 60.931847, Temperature: 848.101\n",
      "Step: 700, E_pot: 25.124086, E_kin: 35.73866, E_tot: 60.862747, Temperature: 856.3996\n",
      "Step: 710, E_pot: 24.830414, E_kin: 36.01023, E_tot: 60.840645, Temperature: 862.9073\n",
      "Step: 720, E_pot: 24.875265, E_kin: 35.910927, E_tot: 60.786194, Temperature: 860.52765\n",
      "Step: 730, E_pot: 25.316078, E_kin: 35.31468, E_tot: 60.630756, Temperature: 846.23987\n",
      "Step: 740, E_pot: 25.898499, E_kin: 35.023613, E_tot: 60.92211, Temperature: 839.2651\n",
      "Step: 750, E_pot: 26.195635, E_kin: 34.879192, E_tot: 61.07483, Temperature: 835.8044\n",
      "Step: 760, E_pot: 26.302698, E_kin: 34.28463, E_tot: 60.587326, Temperature: 821.55695\n",
      "Step: 770, E_pot: 26.331356, E_kin: 34.391502, E_tot: 60.72286, Temperature: 824.1179\n",
      "Step: 780, E_pot: 25.80368, E_kin: 34.925583, E_tot: 60.729263, Temperature: 836.916\n",
      "Step: 790, E_pot: 25.122261, E_kin: 35.54408, E_tot: 60.66634, Temperature: 851.73694\n",
      "Step: 800, E_pot: 24.560558, E_kin: 35.948467, E_tot: 60.509026, Temperature: 861.42725\n",
      "Step: 810, E_pot: 23.654284, E_kin: 36.897964, E_tot: 60.552246, Temperature: 884.1799\n",
      "Step: 820, E_pot: 21.510822, E_kin: 38.822487, E_tot: 60.33331, Temperature: 930.2969\n",
      "Step: 830, E_pot: 18.505508, E_kin: 41.52137, E_tot: 60.02688, Temperature: 994.9698\n",
      "Step: 840, E_pot: 19.043682, E_kin: 40.87612, E_tot: 59.919804, Temperature: 979.50775\n",
      "Step: 850, E_pot: 19.543877, E_kin: 40.22946, E_tot: 59.77334, Temperature: 964.01196\n",
      "Step: 860, E_pot: 18.851387, E_kin: 41.025604, E_tot: 59.87699, Temperature: 983.0898\n",
      "Step: 870, E_pot: 17.841503, E_kin: 41.773003, E_tot: 59.614506, Temperature: 1000.9996\n",
      "Step: 880, E_pot: 16.188223, E_kin: 43.433746, E_tot: 59.62197, Temperature: 1040.7957\n",
      "Step: 890, E_pot: 14.4079075, E_kin: 44.828632, E_tot: 59.23654, Temperature: 1074.2211\n",
      "Step: 900, E_pot: 14.69935, E_kin: 44.42345, E_tot: 59.122803, Temperature: 1064.5118\n",
      "Step: 910, E_pot: 15.883245, E_kin: 44.34402, E_tot: 60.227264, Temperature: 1062.6084\n",
      "Step: 920, E_pot: 15.605328, E_kin: 44.689617, E_tot: 60.294945, Temperature: 1070.89\n",
      "Step: 930, E_pot: 16.117765, E_kin: 45.19873, E_tot: 61.316498, Temperature: 1083.0897\n",
      "Step: 940, E_pot: 16.98919, E_kin: 44.8981, E_tot: 61.88729, Temperature: 1075.8859\n",
      "Step: 950, E_pot: 17.284014, E_kin: 44.848244, E_tot: 62.132256, Temperature: 1074.691\n",
      "Step: 960, E_pot: 17.862278, E_kin: 43.684853, E_tot: 61.54713, Temperature: 1046.813\n",
      "Step: 970, E_pot: 18.075682, E_kin: 43.38553, E_tot: 61.461212, Temperature: 1039.6403\n",
      "Step: 980, E_pot: 17.245943, E_kin: 44.214924, E_tot: 61.46087, Temperature: 1059.515\n",
      "Step: 990, E_pot: 15.864088, E_kin: 46.02162, E_tot: 61.88571, Temperature: 1102.8086\n",
      "Run Time: 00:00:10\n"
     ]
    }
   ],
   "source": [
    "beg_time = time.time()\n",
    "md.run(1000, callbacks=[cb_sim, cb_h5md])\n",
    "end_time = time.time()\n",
    "used_time = end_time - beg_time\n",
    "m, s = divmod(used_time, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"Run Time: %02d:%02d:%02d\" % (h, m, s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4976b8d1b143660084a7ba7652639898bf5b269ba26f14965a18b12288aa8002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
